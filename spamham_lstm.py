# -*- coding: utf-8 -*-
"""spamHam_LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DEu0z8TKwCvLX_aJUoJ6n6BducXjUwUD
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

df = pd.read_csv(r"/content/drive/MyDrive/New/spam.csv", encoding='latin1')

from google.colab import drive
drive.mount('/content/drive')

df.info()

df

# @title v1 vs Unnamed: 4

from matplotlib import pyplot as plt
import seaborn as sns
import pandas as pd
plt.subplots(figsize=(8, 8))
df_2dhist = pd.DataFrame({
    x_label: grp['Unnamed: 4'].value_counts()
    for x_label, grp in df.groupby('v1')
})
sns.heatmap(df_2dhist, cmap='viridis')
plt.xlabel('v1')
_ = plt.ylabel('Unnamed: 4')

# @title Unnamed: 4

from matplotlib import pyplot as plt
import seaborn as sns
df.groupby('Unnamed: 4').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

# Preprocess the data
le = LabelEncoder()
df['v1'] = le.fit_transform(df['v1'])
X_train, X_test, y_train, y_test = train_test_split(df['v2'], df['v1'], test_size=0.2, random_state=42)

# Tokenize and pad sequences
max_words = 1000  # You can adjust this
tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")# out-of-vocabulary
tokenizer.fit_on_texts(X_train)
X_train_sequences = tokenizer.texts_to_sequences(X_train)
X_test_sequences = tokenizer.texts_to_sequences(X_test)

max_sequence_length = 100  # You can adjust this based on your dataset
X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')
X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')

# Build the LSTM model
embedding_dim = 16  # You can adjust this based on your dataset
model = Sequential()
model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))
model.add(LSTM(64))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
epochs = 5  # You can adjust this based on your dataset
model.fit(X_train_padded, y_train, epochs=epochs, validation_data=(X_test_padded, y_test))

# Evaluate the model
loss, accuracy = model.evaluate(X_test_padded, y_test)
print(f'\nTest accuracy: {accuracy * 100:.2f}%')

from sklearn.preprocessing import LabelEncoder

labels = ['spam', 'ham', 'spam', 'ham', 'spam']

# Using fit_transform
le = LabelEncoder()
numerical_labels = le.fit_transform(labels)

# Equivalent to the above, but using fit and transform separately
# le.fit(labels)
# numerical_labels = le.transform(labels)

print(numerical_labels)

# Make predictions on new data
new_texts = ["Congratulations! You've won a free vacation.", "Meeting at 10am tomorrow."]
new_sequences = tokenizer.texts_to_sequences(new_texts)
new_padded = pad_sequences(new_sequences, maxlen=max_sequence_length, padding='post', truncating='post')
predictions = model.predict(new_padded)
predicted_labels = ['spam' if pred > 0.5 else 'ham' for pred in predictions]
print("Predicted labels:", predicted_labels)